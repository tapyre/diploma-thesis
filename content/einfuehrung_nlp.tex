\chapter{Einführung in Natural Language Processing (NLP)}

Natural Language Processing (NLP) ist ein zentraler Bereich der Künstlichen Intelligenz, der sich mit der automatischen Verarbeitung menschlicher Sprache beschäftigt. Ziel ist es, Texte so zu analysieren und zu interpretieren, dass Computer sprachbasierte Aufgaben ausführen können – etwa Suchanfragen beantworten, Texte zusammenfassen oder Dokumente klassifizieren. Moderne Systeme wie Suchmaschinen, Chatbots oder Sprachassistenten bauen maßgeblich auf Methoden des NLP auf \cite{jurafsky2023slp}.

Während frühe Ansätze vor allem statistische Modelle nutzten, basiert das heutige NLP überwiegend auf tiefen neuronalen Netzen. Eine entscheidende Rolle spielt dabei die Frage, wie Bedeutungen mathematisch repräsentiert werden können. Diese Repräsentationen werden als Embeddings bezeichnet.

\section{Klassische NLP-Ansätze}

Vor dem Aufkommen neuronaler Modelle wurden Texte meist mithilfe statistischer Verfahren dargestellt. Typische Beispiele sind Bag-of-Words, TF--IDF und N-Gramme. Diese Methoden berücksichtigen jedoch weder semantische Beziehungen noch Kontextinformationen. So wird nicht erkannt, dass „Auto“ und „Fahrzeug“ ähnliche Bedeutungen haben oder dass „Bank“ sowohl ein Sitzmöbel als auch ein Finanzinstitut bezeichnen kann \cite{jurafsky2023slp}. 

Für einfache Klassifikationsaufgaben sind solche Modelle oft ausreichend, stoßen jedoch bei komplexeren Anwendungen – etwa semantischer Suche oder Übersetzung – schnell an ihre Grenzen.

\section{Einführung in Embeddings}

Da Computer ausschließlich mit numerischen Daten arbeiten, müssen Texte in Zahlen überführt werden. Embeddings lösen dieses Problem, indem sie Wörter, Sätze oder ganze Dokumente als Vektoren in einem kontinuierlichen Raum darstellen. Dabei gilt:

\begin{itemize}
    \item Ähnliche Bedeutungen sollen ähnliche Vektoren besitzen.
    \item Unterschiedliche Bedeutungen sollen weit voneinander entfernt liegen.
    \item Kontextinformationen sollen möglichst berücksichtigt werden.
\end{itemize}

Embeddings bilden die Grundlage vieler moderner NLP-Systeme und ermöglichen semantische Ähnlichkeitsanalysen sowie inhaltliche Textvergleiche.

\section{Word Embeddings: Word2Vec und GloVe}

Einen bedeutenden Fortschritt stellten Word Embeddings wie Word2Vec dar. Diese Modelle ordnen jedem Wort einen festen Vektor zu und basieren auf der Idee, dass Wörter, die in ähnlichen Kontexten auftreten, ähnliche Repräsentationen erhalten. Dadurch entstehen semantische Strukturen wie:

\[
\mathrm{Koenig} - \mathrm{Mann} + \mathrm{Frau} \approx \mathrm{Koenigin}
\]

Word2Vec \cite{mikolov2013word2vec} und ähnliche Ansätze erfassen grundlegende semantische Beziehungen, ignorieren jedoch die Mehrdeutigkeit von Wörtern: Das Wort „Bank“ hat stets denselben Vektor, unabhängig vom Kontext.

\section{Kontextualisierte Embeddings}

Mit tiefen neuronalen Netzen entstanden Modelle, die Wortbedeutungen kontextabhängig repräsentieren. Ein Beispiel dafür ist ELMo, das für jedes Wort unterschiedliche Vektoren erzeugt – abhängig vom Satz, in dem es vorkommt. Diese Ansätze bilden eine Übergangsphase zwischen klassischen Embeddings und modernen Transformer-Modellen.

\section{Embeddings mit der Transformer-Architektur}

Mit der Einführung der Transformer-Architektur wurden neue Maßstäbe gesetzt. Transformer-Encoder wie BERT \cite{devlin2018bert} oder wissenschaftsspezifische Modelle wie SPECTER \cite{cohan2020specter} erzeugen hochqualitative, kontextualisierte Embeddings, indem sie den gesamten Satz oder sogar das gesamte Dokument berücksichtigen.

Dies führt zu:

\begin{itemize}
    \item kontextabhängigen Wortvektoren,
    \item Satz- und Dokumentrepräsentationen als einzelne Vektoren,
    \item präziser semantischer Modellierung,
    \item robuster Erfassung langer und komplexer Zusammenhänge.
\end{itemize}

Transformer-Embeddings sind daher besonders gut geeignet, um wissenschaftliche Texte mit ihren komplexen Begrifflichkeiten und Strukturmerkmalen zu verarbeiten.

\section{Relevanz für Tapyre Paper Search}

Im Projekt \textit{Tapyre Paper Search} dienen Embeddings dazu, wissenschaftliche Artikel in einem Vektorraum abzubilden. Dokumente mit thematischen Ähnlichkeiten liegen darin räumlich nahe beieinander, was eine präzise semantische Suche ermöglicht. Spezialisierte Modelle wie SPECTER2, die auf wissenschaftlichen Publikationen trainiert wurden, verbessern die Erkennung fachlicher Zusammenhänge nochmals deutlich.

